{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(filename, data):\n",
    "    #Storing data with labels\n",
    "    a_file = open(filename, \"wb\")\n",
    "    pickle.dump(data, a_file)\n",
    "    a_file.close()\n",
    "    \n",
    "\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('train_14k_split_conll.txt', 'r')\n",
    "sentiment_dataset = []\n",
    "\n",
    "start = False\n",
    "for i, line in enumerate(f):\n",
    "    line = line.split('\\t')\n",
    "    \n",
    "    if len(line) == 3:\n",
    "        start = True\n",
    "        tweet = ''\n",
    "        tweet_id = int(line[1])\n",
    "        label = line[2][:-1]\n",
    "    elif len(line) == 1:\n",
    "        start = False\n",
    "        tweet = tweet[:-1]\n",
    "        sentiment_dataset.append((tweet_id, tweet, label))\n",
    "    \n",
    "    if start and len(line) != 3:\n",
    "        if line[0] == '@' or line[0] == '#':\n",
    "            tweet += line[0]\n",
    "        else:\n",
    "            tweet += line[0] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('sentiment_data_train.pkl', sentiment_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "###removing links present after long tweets ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_long_processed_dataset = []\n",
    "for i, (tweet_id, tweet, label) in enumerate(sentiment_dataset):\n",
    "    tweet = tweet.split()\n",
    "    check = [(ord(l),j) for j,l in enumerate(tweet) if len(l) == 1]\n",
    "    \n",
    "    #check if ... is in the tweet\n",
    "    if len(check) > 0:\n",
    "        if 8230 in np.array(check)[:,0]:\n",
    "            index = np.where(np.array(check)[:,0] == 8230)[0][0]\n",
    "            too_long_index = check[index][1]\n",
    "\n",
    "            if len(tweet) > too_long_index + 1 and tweet[too_long_index+1] == 'https':\n",
    "                tweet = tweet[:too_long_index + 1]\n",
    "\n",
    "    #combine tweet into string and add to dataset      \n",
    "    processed_tweet = ''\n",
    "    for l in tweet:\n",
    "        processed_tweet += l + ' '\n",
    "    processed_tweet = processed_tweet[:-1]\n",
    "\n",
    "    too_long_processed_dataset.append((tweet_id, processed_tweet, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('sentiment_data_train.pkl', too_long_processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining other links in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672 0.11942857142857143\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, (tweet_id, tweet, label) in enumerate(too_long_processed_dataset):\n",
    "    tweet = tweet.split()\n",
    "    if 'https' in tweet:\n",
    "        counter += 1\n",
    "        \n",
    "print(counter, counter/14000)## 12% tweets still have lnks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_links = []\n",
    "for i, (tweet_id, tweet, label) in enumerate(too_long_processed_dataset):\n",
    "    tweet = tweet.split()\n",
    "\n",
    "    if 'https' in tweet:\n",
    "        index = tweet.index('https')\n",
    "        \n",
    "        link = ''\n",
    "        if '.' in tweet[index:index + 7]:\n",
    "            link_len = 7\n",
    "        else:\n",
    "            link_len = 6\n",
    "        \n",
    "        try:\n",
    "            for i in range(link_len):\n",
    "                link += tweet[index]\n",
    "                tweet.pop(index)\n",
    "        except:\n",
    "            tweet.insert(index, link)\n",
    "            \n",
    "        tweet.insert(index, link)\n",
    "    \n",
    "    #combine tweet into string and add to dataset\n",
    "    processed_tweet = ''\n",
    "    for l in tweet:\n",
    "        processed_tweet += l + ' '\n",
    "    processed_tweet = processed_tweet[:-1]\n",
    "    \n",
    "    handle_links.append((tweet_id, processed_tweet, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data('sentiment_data_train.pkl', handle_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
