{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(filename, data):\n",
    "    #Storing data with labels\n",
    "    a_file = open(filename, \"wb\")\n",
    "    pickle.dump(data, a_file)\n",
    "    a_file.close()\n",
    "    \n",
    "\n",
    "def load_data(filename):\n",
    "    a_file = open(filename, \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = '0'\n",
    "current_directory = 'iteration' + iteration + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data( current_directory + 'iteration_' + iteration + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.5092857142857142\n"
     ]
    }
   ],
   "source": [
    "#find overall accuracy:\n",
    "overall_accuracy = np.array([pred_label == label for (sentence, pred_label, label, score) in data])\n",
    "print('Overall Accuracy:', np.sum(overall_accuracy)/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: True\n"
     ]
    }
   ],
   "source": [
    "#Store in different dictionaries based on actual label\n",
    "pred_positive = []\n",
    "pred_negative = []\n",
    "pred_neutral = []\n",
    "for (sentence, pred_label, label, score) in data:\n",
    "    if pred_label == 'positive':\n",
    "        pred_positive.append((sentence, pred_label, label, score))\n",
    "    elif pred_label == 'negative':\n",
    "        pred_negative.append((sentence, pred_label, label, score))\n",
    "    elif pred_label == 'neutral':\n",
    "        pred_neutral.append((sentence, pred_label, label, score))\n",
    "        \n",
    "print('Check:', (len(pred_positive) + len(pred_negative) + len(pred_neutral)) == len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sorting by predicting confidence\n",
    "sorted_pred_positive = sorted(pred_positive, key=lambda k: k[3], reverse=True)\n",
    "sorted_pred_negative = sorted(pred_negative, key=lambda k: k[3], reverse=True)\n",
    "sorted_pred_neutral = sorted(pred_neutral, key=lambda k: k[3], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Total number of samples in top10% high confidence predictions for each class\n",
    "n_top10_positive = len(pred_positive)//10\n",
    "n_top10_negative = len(pred_negative)//10\n",
    "n_top10_neutral = len(pred_neutral)//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_confidence_positive = sorted_pred_positive[:n_top10_positive]\n",
    "save_data(current_directory + 'predicted_positive_top10_conf_' + iteration + '.pkl', top_confidence_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_confidence_negative = sorted_pred_negative[:n_top10_negative]\n",
    "save_data(current_directory + 'predicted_negative_top10_conf_' + iteration + '.pkl', top_confidence_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "###saving neutral sentences with right distribution choice\n",
    "top_confidence_neutral = []\n",
    "for (sentence, pred_label, label, score) in sorted_pred_neutral:\n",
    "    if score > 0.8 and score < 0.85:\n",
    "        if pred_label == 'neutral':\n",
    "            top_confidence_neutral.append((sentence, pred_label, label, score))\n",
    "            \n",
    "\n",
    "    if len(top_confidence_neutral) == n_top10_neutral:\n",
    "        break\n",
    "\n",
    "save_data(current_directory + 'predicted_neutral_top10_conf'  + iteration + '.pkl', top_confidence_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new train data set that does not have saved sentences\n",
    "new_directory = 'iteration' + str(int(iteration) + 1)\n",
    "os.mkdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: True\n"
     ]
    }
   ],
   "source": [
    "fine_tune_data = top_confidence_positive + top_confidence_negative + top_confidence_neutral\n",
    "random.shuffle(fine_tune_data)\n",
    "print('Check:', len(fine_tune_data) == len(top_confidence_positive) + len(top_confidence_negative) + len(top_confidence_neutral))\n",
    "\n",
    "save_data(new_directory + '/fine_tune_'  + str(int(iteration) + 1) + '.pkl', fine_tune_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "###collect all chosen sentences together\n",
    "chosen_sentences = []\n",
    "for (sentence, pred_label, label, score) in top_confidence_positive:\n",
    "    chosen_sentences.append(sentence)\n",
    "    \n",
    "for (sentence, pred_label, label, score) in top_confidence_negative:\n",
    "    chosen_sentences.append(sentence)\n",
    "    \n",
    "for (sentence, pred_label, label, score) in top_confidence_neutral:\n",
    "    chosen_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = load_data(current_directory + 'processed_data_train_' + iteration + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_iteration_data = []\n",
    "for (tweet_id, tweet, label) in original_dataset:\n",
    "    if tweet not in chosen_sentences:\n",
    "        next_iteration_data.append((tweet_id, tweet, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data( new_directory + '/processed_data_train_' + str(int(iteration) + 1) + '.pkl', next_iteration_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
